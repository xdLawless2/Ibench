# IBench

A visual reasoning benchmark for LLMs that tests their ability to count intersection points in line diagrams.

## Features

- **Parallel Processing**: Tests multiple models concurrently for fast benchmarking
- **Reasoning Model Support**: Allocates sufficient tokens for models that think before answering
- **Comprehensive Debugging**: Saves all model responses for analysis
- **Real-time Progress**: Live progress bars and accuracy tracking
- **Visual Results**: Generates accuracy charts and leaderboards

## Setup

### Requirements
- Python 3.10+
- OpenRouter API key

### Installation

```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set your OpenRouter API key
export OPENROUTER_API_KEY=sk-or-...
```

## Usage

### 1. Prepare the benchmark data

```bash
python3 src/prepare.py
```

This validates the images and ground truth labels, creating `data/manifest.jsonl`.

### 2. Run the benchmark

```bash
# Run benchmark with all models in models.txt
python3 src/run_benchmark.py

# Customize concurrency settings for faster processing
python3 src/run_benchmark.py --max-models-concurrent 5 --max-images-concurrent 10 --rps 20
```

## Command Line Options

- `--max-models-concurrent N`: Number of models to process in parallel (default: 3)
- `--max-images-concurrent N`: Number of images per model to process in parallel (default: 5)
- `--rps N`: API requests per second rate limit (default: 10)
- `--debug-dir PATH`: Directory for debug output (default: debug_output)
- `--out PATH`: Output path for accuracy chart (default: outputs/accuracy.png)

## Output

The benchmark produces:

1. **Live Progress**: Real-time progress bars showing model and image processing
2. **Accuracy Scores**: Immediate feedback as each model completes
3. **Debug Files**: Full model responses saved to `debug_output/<model>/<image>.json`
4. **Leaderboard**: Final rankings with accuracy percentages
5. **Visualization**: Bar chart comparing all models saved to `outputs/accuracy.png`

## File Structure

```
IBench/
├── imgs/                    # Test images (1.png through 20.png)
├── src/
│   ├── prepare.py          # Data preparation script
│   ├── run_benchmark.py    # Main benchmark runner
│   ├── models.txt          # List of OpenRouter model slugs
│   └── truth.txt           # Ground truth labels (1-6)
├── data/
│   └── manifest.jsonl      # Generated by prepare.py
├── debug_output/           # Model responses (auto-generated)
│   ├── <model>/
│   │   ├── 1.json
│   │   ├── 2.json
│   │   └── _summary.json
└── outputs/
    └── accuracy.png        # Generated accuracy chart
```

## Performance

- Sequential processing: ~6 hours for 17 models × 20 images
- Parallel processing: ~30-60 seconds with default settings
- Adjustable concurrency to match your API rate limits

## Notes

- Models are given up to 8192 tokens to complete their reasoning
- Only integers 1-6 are considered valid answers
- Models that output explanations before answering are handled correctly
- Debug output is cleared on each run for fresh results
